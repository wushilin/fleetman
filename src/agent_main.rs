mod agent_config;
mod agent_state;
mod agent_s3;
mod s3_retry;
mod processmaster_client;
// NOTE: systemd unit files are generated by Fleetman into the deployment as `app.service`,
// then installed and rendered (fleetagent_* tokens) by FleetAgent at deploy time.

use agent_config::*;
use agent_state::*;
use anyhow::{Context, Result};
use aws_sdk_s3::Client as S3Client;
use aws_sdk_s3::primitives::ByteStream;
use regex::{NoExpand, Regex};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs;
use std::os::unix::fs::PermissionsExt;
use std::path::{Path, PathBuf};
use std::process::Command;
use std::sync::{Arc, Mutex};
use std::time::{SystemTime, UNIX_EPOCH};
use tokio::time::{sleep, Duration, Instant, MissedTickBehavior};
use tracing::{debug, error, info, warn};

use s3_retry::retry_s3_operation;

fn apply_fleetagent_tokens_in_file(file_path: &Path, cell_id: &str, working_dir_abs: &str) -> Result<()> {
    if !file_path.exists() {
        anyhow::bail!("Required file not found: {}", file_path.display());
    }
    let content = fs::read_to_string(file_path)
        .with_context(|| format!("Failed to read {:?}", file_path))?;

    let re_cell = Regex::new(r"\{\{\s*fleetagent_cell_id\s*\}\}").unwrap();
    let re_wd = Regex::new(r"\{\{\s*fleetagent_service_working_dir\s*\}\}").unwrap();
    let re_base = Regex::new(r"\{\{\s*fleetagent_base_dir\s*\}\}").unwrap();

    let base_dir_abs = std::env::current_dir()
        .unwrap_or_else(|_| PathBuf::from("/"))
        .to_string_lossy()
        .to_string();

    let out = re_base
        .replace_all(
            &re_wd
                .replace_all(
                    &re_cell.replace_all(&content, NoExpand(cell_id)).to_string(),
                    NoExpand(working_dir_abs),
                )
                .to_string(),
            NoExpand(base_dir_abs.as_str()),
        )
        .to_string();

    if out != content {
        fs::write(file_path, out.as_bytes())
            .with_context(|| format!("Failed to write {:?}", file_path))?;
    }

    Ok(())
}

fn apply_fleetagent_tokens_in_file_optional(file_path: &Path, cell_id: &str, working_dir_abs: &str) -> Result<()> {
    if !file_path.exists() {
        // Optional template files might not exist on some deployments.
        return Ok(());
    }
    apply_fleetagent_tokens_in_file(file_path, cell_id, working_dir_abs)
}

fn apply_templates_from_manifest(target_dir: &Path, cell_id: &str, working_dir_abs: &str, manifest: &ManifestSnapshot) -> Result<()> {
    for f in &manifest.files {
        if f.file_type.to_lowercase() != "template" {
            continue;
        }
        // Template files are editable text files rendered by FleetAgent at deploy time.
        // Apply fleetagent_* tokens before ownership/permissions are applied.
        let p = target_dir.join(&f.path);
        apply_fleetagent_tokens_in_file_optional(&p, cell_id, working_dir_abs)?;
    }
    Ok(())
}

fn apply_templates_from_overrides_manifest(service_dir: &Path, cell_id: &str, working_dir_abs: &str) -> Result<()> {
    let manifest_path = service_dir.join("overrides_manifest.json");
    if !manifest_path.exists() {
        return Ok(());
    }
    let content = fs::read_to_string(&manifest_path)
        .with_context(|| format!("Failed to read overrides manifest at {:?}", manifest_path))?;
    let manifest: CellOverridesManifest = serde_json::from_str(&content)
        .with_context(|| format!("Failed to parse overrides manifest at {:?}", manifest_path))?;

    for f in &manifest.files {
        if f.file_type.to_lowercase() != "template" {
            continue;
        }
        let p = service_dir.join(&f.path);
        apply_fleetagent_tokens_in_file_optional(&p, cell_id, working_dir_abs)?;
    }
    Ok(())
}

fn apply_service_identity_in_file(file_path: &Path, service_user: &str, service_group: &str) -> Result<()> {
    if !file_path.exists() {
        anyhow::bail!("Required file not found: {}", file_path.display());
    }
    let content = fs::read_to_string(file_path)
        .with_context(|| format!("Failed to read {:?}", file_path))?;

    let file_name = file_path.file_name().and_then(|s| s.to_str()).unwrap_or("");
    let is_systemd_unit = file_name.ends_with(".service");
    let is_pm_yml = file_name == "service.yml" || file_name == "service.yaml";

    // Systemd: preserve leading whitespace if any.
    let re_sd_user = Regex::new(r"^(\s*)User\s*=.*$").unwrap();
    let re_sd_group = Regex::new(r"^(\s*)Group\s*=.*$").unwrap();

    // ProcessMaster yaml: preserve indentation.
    let re_pm_user = Regex::new(r"^(\s*)user:\s*.*$").unwrap();
    let re_pm_group = Regex::new(r"^(\s*)group:\s*.*$").unwrap();

    let mut out_lines: Vec<String> = Vec::new();
    let mut changed = false;

    for line in content.lines() {
        let trimmed = line.trim_start();
        // Do not rewrite comment-only lines (preserves user edits + avoids touching commented examples).
        if trimmed.starts_with('#') || trimmed.starts_with(';') {
            out_lines.push(line.to_string());
            continue;
        }

        if is_systemd_unit {
            if let Some(caps) = re_sd_user.captures(line) {
                let indent = caps.get(1).map(|m| m.as_str()).unwrap_or("");
                out_lines.push(format!("{indent}User={service_user}"));
                changed = true;
                continue;
            }
            if let Some(caps) = re_sd_group.captures(line) {
                let indent = caps.get(1).map(|m| m.as_str()).unwrap_or("");
                out_lines.push(format!("{indent}Group={service_group}"));
                changed = true;
                continue;
            }
        } else if is_pm_yml {
            if let Some(caps) = re_pm_user.captures(line) {
                let indent = caps.get(1).map(|m| m.as_str()).unwrap_or("");
                out_lines.push(format!("{indent}user: {service_user}"));
                changed = true;
                continue;
            }
            if let Some(caps) = re_pm_group.captures(line) {
                let indent = caps.get(1).map(|m| m.as_str()).unwrap_or("");
                out_lines.push(format!("{indent}group: {service_group}"));
                changed = true;
                continue;
            }
        }

        out_lines.push(line.to_string());
    }

    if changed {
        let mut out = out_lines.join("\n");
        // Preserve trailing newline if it existed.
        if content.ends_with('\n') {
            out.push('\n');
        }
        fs::write(file_path, out.as_bytes())
            .with_context(|| format!("Failed to write {:?}", file_path))?;
    }

    Ok(())
}

#[tokio::main]
async fn main() -> Result<()> {
    // Load configuration
    let config_path = std::env::args()
        .nth(1)
        .unwrap_or_else(|| "agent_config.yaml".to_string());

    let config_contents = fs::read_to_string(&config_path)
        .with_context(|| format!("Failed to read config file: {}", config_path))?;

    let config: AgentConfig = serde_yaml_ng::from_str(&config_contents)
        .with_context(|| "Failed to parse config file")?;

    // Initialize tracing (guard must be kept alive)
    let _guard = init_tracing(&config.global);

    info!("üöÄ Fleetagent starting...");
    info!("Node ID: {}", config.global.node_id);

    // Initialize S3 client
    let s3_client = agent_s3::create_s3_client(&config.object_storage).await;

    // Load or create state
    let state_file = PathBuf::from(format!(".agent_state_{}.json", config.global.node_id));
    let state = AgentState::load_from_file(&state_file).unwrap_or_else(|e| {
        warn!("Failed to load state file: {}, starting fresh", e);
        AgentState::new()
    });
    let shared_state = Arc::new(Mutex::new(state));

    // On startup, sync state from host reality (systemd units in service_folder, supervisor conf,
    // deploy folders) so we don't report stale deployments from the local state file.
    if let Err(e) = sync_state_from_running_services(&config, &shared_state) {
        warn!("Failed to sync state from host: {}", e);
    }

    info!("‚úÖ Agent initialized, entering main loop");

    // Background: publish agent heartbeat every minute for the Fleetman controller UI.
    // S3 key: <prefix>/agents/<node_id>.json
    {
        let node_id = config.global.node_id.clone();
        let config_for_report = config.clone();
        let object_storage = config.object_storage.clone();
        let retry_count = config.global.s3_retry_count;
        let retry_delay_ms = config.global.s3_retry_delay_ms;
        let s3_client = s3_client.clone();

        tokio::spawn(async move {
            // Fixed-rate heartbeat (doesn't drift based on upload duration).
            // If the process is paused / overloaded, skip missed ticks rather than "catch up"
            // with a burst of back-to-back uploads.
            let period = Duration::from_secs(60);
            let mut interval = tokio::time::interval_at(Instant::now() + period, period);
            interval.set_missed_tick_behavior(MissedTickBehavior::Skip);

            // Emit once immediately on startup (best-effort), then continue on the fixed cadence.
            if let Err(e) = write_agent_status_json(
                &config_for_report,
                &object_storage,
                &s3_client,
                &node_id,
                retry_count,
                retry_delay_ms,
            )
            .await
            {
                warn!("Failed to write initial agent heartbeat to S3: {}", e);
            }

            loop {
                interval.tick().await;
                if let Err(e) = write_agent_status_json(
                    &config_for_report,
                    &object_storage,
                    &s3_client,
                    &node_id,
                    retry_count,
                    retry_delay_ms,
                )
                .await
                {
                    warn!("Failed to write agent heartbeat to S3: {}", e);
                }
            }
        });
    }

    // Main loop
    loop {
        info!("üîç Starting discovery cycle...");
        
        match run_agent_cycle(&config, &s3_client, &shared_state).await {
            Ok(_) => info!("‚úÖ Agent cycle completed successfully"),
            Err(e) => error!("‚ùå Error in agent cycle: {}", e),
        }

        // Save state
        if let Err(e) = shared_state.lock().unwrap().save_to_file(&state_file) {
            error!("Failed to save state: {}", e);
        }

        info!("‚è≥ Sleeping for {}ms before next check", config.global.check_interval_ms);
        sleep(Duration::from_millis(config.global.check_interval_ms)).await;
    }
}

// -------------------- Deployed-folder manifest (for restart accuracy) --------------------

// Deployed-folder manifest used for restart reconciliation and UI reporting.
// Per requirement: this file is named `manifest.json` in the deployed folder.
const LOCAL_DEPLOYED_MANIFEST_FILE: &str = "manifest.json";

#[derive(Debug, Clone, Serialize, Deserialize)]
struct DeployedManifestFile {
    // Controller snapshot fields (these exist in deployments/<manifest>/<version>/manifest.json)
    #[serde(default)]
    manifest_name: String,
    #[serde(default)]
    version: String,
    #[serde(default)]
    profile: String,
}

fn read_deployed_manifest(service_dir: &str) -> Option<DeployedManifestFile> {
    let path = PathBuf::from(service_dir).join(LOCAL_DEPLOYED_MANIFEST_FILE);
    let content = fs::read_to_string(path).ok()?;
    serde_json::from_str::<DeployedManifestFile>(&content).ok()
}

fn write_deployed_manifest(service_dir: &str, manifest_name: &str, version: &str, profile: &str) -> Result<()> {
    let path = PathBuf::from(service_dir).join(LOCAL_DEPLOYED_MANIFEST_FILE);
    let deployed = DeployedManifestFile {
        manifest_name: manifest_name.to_string(),
        version: version.to_string(),
        profile: profile.to_string(),
    };
    let content = serde_json::to_string_pretty(&deployed)
        .with_context(|| "Failed to serialize deployed manifest")?;
    fs::write(&path, content.as_bytes())
        .with_context(|| format!("Failed to write deployed manifest to {:?}", path))?;
    Ok(())
}

// -------------------- Host observation (startup sync + heartbeat) --------------------

#[derive(Debug, Clone)]
struct ObservedCell {
    cell_id: String,
    manifest_name: String,
    profile: String,
    version: Option<String>,
    running: Option<bool>, // None => N/A (deploy)
}

fn sync_state_from_running_services(config: &AgentConfig, shared_state: &SharedState) -> Result<()> {
    // Preserve trigger timestamps/previous version from state file to avoid redeploy storms.
    let previous_cells = {
        let state = shared_state.lock().unwrap();
        state.cells.clone()
    };

    let observed = observe_host_cells(config)?;
    let mut discovered: HashMap<String, CellState> = HashMap::new();
    for o in observed {
        let prev = previous_cells.get(&o.cell_id);
        discovered.insert(
            o.cell_id.clone(),
            CellState {
                cell_id: o.cell_id.clone(),
                manifest_name: o.manifest_name.clone(),
                profile: o.profile.clone(),
                current_version: o.version.clone(),
                previous_version: prev.and_then(|p| p.previous_version.clone()),
                trigger_timestamp: prev.and_then(|p| p.trigger_timestamp),
                last_deployment_status: match o.running {
                    Some(true) => DeploymentStatus::Success,
                    Some(false) => DeploymentStatus::Unknown,
                    None => DeploymentStatus::Success, // deploy: applied, runtime N/A
                },
                last_deployment_time: prev.and_then(|p| p.last_deployment_time),
                running: o.running,
            },
        );
    }

    {
        let mut state = shared_state.lock().unwrap();
        state.cells = discovered;
    }

    Ok(())
}

fn observe_host_cells(config: &AgentConfig) -> Result<Vec<ObservedCell>> {
    let mut out: HashMap<String, ObservedCell> = HashMap::new();

    // systemd: enumerate unit files in service_folder, then check running status
    if let Some(systemd_cfg) = &config.systemd {
        let units = discover_systemd_units_from_folder(systemd_cfg)?;
        for (unit, cell_id) in units {
            let running = is_systemd_unit_running(systemd_cfg, &unit).ok();
            // Folder name matches unit/service name: fleetman-<cell_id>
            let service_dir = format!("{}/fleetman-{}", systemd_cfg.working_directory, cell_id);
            let manifest = read_deployed_manifest(&service_dir);
            let (manifest_name, profile, version) = match manifest {
                Some(m) => (
                    if m.manifest_name.is_empty() { "unknown".to_string() } else { m.manifest_name },
                    if m.profile.is_empty() { "systemd".to_string() } else { m.profile },
                    if m.version.is_empty() { None } else { Some(m.version) },
                ),
                None => ("unknown".to_string(), "systemd".to_string(), None),
            };
            out.insert(
                cell_id.clone(),
                ObservedCell {
                    cell_id,
                    manifest_name,
                    profile,
                    version,
                    running,
                },
            );
        }
    }

    // processmaster: enumerate auto-services folders, then check pmctl status
    if let Some(pm_cfg) = &config.processmaster {
        let auto_root = PathBuf::from(&pm_cfg.auto_services_root);
        if auto_root.exists() {
            let client = processmaster_client::ProcessMasterClient::new(
                PathBuf::from(&pm_cfg.pmctl_path),
                pm_cfg.sock.clone(),
            );

            for entry in fs::read_dir(&auto_root)
                .with_context(|| format!("Failed to read processmaster auto_services_root {:?}", auto_root))?
            {
                let entry = entry?;
                let path = entry.path();
                if !path.is_dir() {
                    continue;
                }
                let folder = match path.file_name().and_then(|s| s.to_str()) {
                    Some(s) => s.to_string(),
                    None => continue,
                };
                if !folder.starts_with("fleetman-") {
                    continue;
                }
                let cell_id = folder.trim_start_matches("fleetman-").to_string();
                if cell_id.is_empty() {
                    continue;
                }

                let service_dir = path.to_string_lossy().to_string();
                let manifest = read_deployed_manifest(&service_dir);
                let (manifest_name, profile, version) = match manifest {
                    Some(m) => (
                        if m.manifest_name.is_empty() { "unknown".to_string() } else { m.manifest_name },
                        if m.profile.is_empty() { "processmaster".to_string() } else { m.profile },
                        if m.version.is_empty() { None } else { Some(m.version) },
                    ),
                    None => ("unknown".to_string(), "processmaster".to_string(), None),
                };

                let app_name = folder.clone(); // fleetman-<cell_id>
                let running = match client.status_json(&app_name) {
                    Ok(st) => {
                        if !st.ok {
                            Some(false)
                        } else {
                            st.statuses.get(0).map(|s| s.running).or(Some(false))
                        }
                    }
                    Err(_) => Some(false),
                };

                out.insert(
                    cell_id.clone(),
                    ObservedCell {
                        cell_id,
                        manifest_name,
                        profile,
                        version,
                        running,
                    },
                );
            }
        }
    }

    // supervisor support removed (replaced by processmaster).

    // deploy: folders only; running is N/A
    if let Some(deploy_cfg) = &config.deploy {
        let base = PathBuf::from(&deploy_cfg.target_folder);
        if base.exists() {
            for entry in fs::read_dir(&base)
                .with_context(|| format!("Failed to read deploy target folder {:?}", base))?
            {
                let entry = entry?;
                let path = entry.path();
                if !path.is_dir() {
                    continue;
                }
                let cell_id = match path.file_name().and_then(|s| s.to_str()) {
                    Some(s) => s.to_string(),
                    None => continue,
                };
                let service_dir = path.to_string_lossy().to_string();
                let Some(m) = read_deployed_manifest(&service_dir) else { continue };
                if m.profile.to_lowercase() != "deploy" {
                    continue;
                }
                out.insert(
                    cell_id.clone(),
                    ObservedCell {
                        cell_id,
                        manifest_name: if m.manifest_name.is_empty() { "unknown".to_string() } else { m.manifest_name },
                        profile: if m.profile.is_empty() { "deploy".to_string() } else { m.profile },
                        version: if m.version.is_empty() { None } else { Some(m.version) },
                        running: None,
                    },
                );
            }
        }
    }

    Ok(out.into_values().collect())
}

fn discover_systemd_units_from_folder(systemd_cfg: &SystemdConfig) -> Result<Vec<(String, String)>> {
    let dir = PathBuf::from(&systemd_cfg.service_folder);
    if !dir.exists() {
        return Ok(Vec::new());
    }

    let mut units = Vec::new();
    for entry in fs::read_dir(&dir).with_context(|| format!("Failed to read systemd service folder {:?}", dir))? {
        let entry = entry?;
        let path = entry.path();
        if !path.is_file() {
            continue;
        }
        if path.extension().and_then(|s| s.to_str()) != Some("service") {
            continue;
        }
        let unit = match path.file_name().and_then(|s| s.to_str()) {
            Some(s) => s.to_string(),
            None => continue,
        };
        if !unit.starts_with("fleetman-") {
            continue;
        }
        let cell_id = unit
            .trim_end_matches(".service")
            .trim_start_matches("fleetman-")
            .to_string();
        if cell_id.is_empty() {
            continue;
        }
        units.push((unit, cell_id));
    }
    Ok(units)
}

fn is_systemd_unit_running(systemd_cfg: &SystemdConfig, unit: &str) -> Result<bool> {
    let output = Command::new(systemd_cfg.systemctl_path())
        .args(["is-active", "--no-pager", unit])
        .output()
        .with_context(|| "Failed to execute systemctl is-active")?;

    let stdout = String::from_utf8_lossy(&output.stdout);
    Ok(stdout.trim() == "active")
}

// supervisor support removed (replaced by processmaster).

#[derive(Debug, Clone, Serialize)]
struct AgentReportedCell {
    cell_id: String,
    manifest_name: String,
    profile: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    version: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    running: Option<bool>, // None => N/A (deploy)
}

#[derive(Debug, Clone, Serialize)]
struct AgentStatusReport {
    report_timestamp: u64,
    node_id: String,
    cells: Vec<AgentReportedCell>,
}

async fn write_agent_status_json(
    config: &AgentConfig,
    object_storage: &ObjectStorageConfig,
    s3_client: &S3Client,
    node_id: &str,
    retry_count: u32,
    retry_delay_ms: u64,
) -> Result<()> {
    let report_timestamp = current_timestamp();

    let observed = observe_host_cells(config).unwrap_or_else(|e| {
        warn!("Failed to observe host cells for heartbeat: {}", e);
        Vec::new()
    });

    let cells: Vec<AgentReportedCell> = observed
        .into_iter()
        .map(|c| AgentReportedCell {
            cell_id: c.cell_id,
            manifest_name: c.manifest_name,
            profile: c.profile,
            version: c.version,
            running: c.running,
        })
        .collect();

    let report = AgentStatusReport {
        report_timestamp,
        node_id: node_id.to_string(),
        cells,
    };

    let content = serde_json::to_string_pretty(&report)?;
    let key_base = format!("agents/{}.json", node_id);
    let key = object_storage.full_key(&key_base);

    retry_s3_operation(
        &format!("write_agent_status_{}", node_id),
        retry_count,
        retry_delay_ms,
        || async {
            s3_client
                .put_object()
                .bucket(&object_storage.bucket)
                .key(&key)
                .body(ByteStream::from(content.clone().into_bytes()))
                .send()
                .await
        },
    )
    .await
    .map_err(|e| anyhow::anyhow!("Failed to put agent status object: {:?}", e))?;

    Ok(())
}

async fn run_agent_cycle(
    config: &AgentConfig,
    s3_client: &S3Client,
    shared_state: &SharedState,
) -> Result<()> {
    // Discover cells for this node
    info!("üîé Scanning S3 for cells under node: {}", config.global.node_id);
    let cells = discover_cells(config, s3_client).await?;

    // Before processing cells, cleanup any local resources for cells that no longer exist in S3.
    // This is a "converge to declared state" behavior: if controller removed a cell (and archived its
    // S3 folder), the agent should remove local deployed resources.
    if let Err(e) = cleanup_removed_cells(config, s3_client, shared_state, &cells).await {
        warn!("Cleanup removed cells failed (continuing): {:#}", e);
    }

    if cells.is_empty() {
        info!(
            "‚ÑπÔ∏è  No cells found for node {} (bucket: {}, prefix: cells/{}/))",
            config.global.node_id,
            config.object_storage.bucket,
            config.global.node_id
        );
        return Ok(());
    }

    info!("üì° Discovered {} cell(s): {:?}", cells.len(), 
          cells.iter().map(|c| &c.cell_id).collect::<Vec<_>>());

    // Process each cell
    for cell_info in cells {
        match process_cell(config, s3_client, shared_state, &cell_info).await {
            Ok(_) => {}
            Err(e) => error!("Failed to process cell {}: {}", cell_info.cell_id, e),
        }
    }

    Ok(())
}

fn now_ts_for_backup() -> String {
    // yyyyMMdd-HHmmss-SSS
    chrono::Local::now().format("%Y%m%d-%H%M%S-%3f").to_string()
}

fn move_dir_to_backup(src: &Path, backups_dir: &Path, backup_name: &str) -> Result<PathBuf> {
    fs::create_dir_all(backups_dir)
        .with_context(|| format!("Failed to create backups dir {}", backups_dir.display()))?;
    let dst = backups_dir.join(backup_name);
    if dst.exists() {
        anyhow::bail!("Backup destination already exists: {}", dst.display());
    }
    match fs::rename(src, &dst) {
        Ok(_) => Ok(dst),
        Err(e) if e.raw_os_error() == Some(libc::EXDEV) => {
            // Cross-device rename: copy recursively then remove.
            copy_dir_all(src, &dst)?;
            fs::remove_dir_all(src).with_context(|| format!("Failed to remove {}", src.display()))?;
            Ok(dst)
        }
        Err(e) => Err(anyhow::anyhow!("Failed to move {} -> {}: {}", src.display(), dst.display(), e)),
    }
}

fn copy_dir_all(src: &Path, dst: &Path) -> Result<()> {
    fs::create_dir_all(dst).with_context(|| format!("Failed to create {}", dst.display()))?;
    for entry in fs::read_dir(src).with_context(|| format!("Failed to read {}", src.display()))? {
        let entry = entry?;
        let ty = entry.file_type()?;
        let from = entry.path();
        let to = dst.join(entry.file_name());
        if ty.is_dir() {
            copy_dir_all(&from, &to)?;
        } else {
            fs::copy(&from, &to).with_context(|| format!("Failed to copy {} -> {}", from.display(), to.display()))?;
        }
    }
    Ok(())
}

async fn cell_exists_in_s3(config: &AgentConfig, s3_client: &S3Client, cell_id: &str) -> bool {
    // Existence is defined by trigger.json present under cells/<node>/<cell>/.
    let key = config
        .object_storage
        .full_key(&format!("cells/{}/{}/trigger.json", config.global.node_id, cell_id));
    let op = format!("head_trigger_{}", cell_id);
    retry_s3_operation(
        &op,
        config.global.s3_retry_count,
        config.global.s3_retry_delay_ms,
        || async {
            s3_client
                .head_object()
                .bucket(&config.object_storage.bucket)
                .key(&key)
                .send()
                .await
        },
    )
    .await
    .is_ok()
}

async fn cleanup_removed_cells(
    config: &AgentConfig,
    s3_client: &S3Client,
    shared_state: &SharedState,
    discovered_cells: &[CellInfo],
) -> Result<()> {
    let discovered_set: std::collections::HashSet<String> =
        discovered_cells.iter().map(|c| c.cell_id.clone()).collect();

    // Observe what's currently deployed on the host.
    let observed = observe_host_cells(config)?;
    if observed.is_empty() {
        return Ok(());
    }

    let base_dir = std::env::current_dir().unwrap_or_else(|_| PathBuf::from("/"));
    // Admin-controlled backups: we never rm -rf service data on cell removal.
    let backups_dir = base_dir.join("backup");

    for c in observed {
        if discovered_set.contains(&c.cell_id) {
            continue;
        }

        // Safety check: confirm cell folder truly gone in S3.
        if cell_exists_in_s3(config, s3_client, &c.cell_id).await {
            continue;
        }

        match c.profile.to_lowercase().as_str() {
            "deploy" => {
                if let Some(deploy_cfg) = &config.deploy {
                    let target_dir = PathBuf::from(&deploy_cfg.target_folder).join(&c.cell_id);
                    if target_dir.exists() {
                        let ts = now_ts_for_backup();
                        let backup_name = format!("fleetman-{}-{}", c.cell_id, ts);
                        info!(
                            "üßπ Cell removed: moving deploy folder {} -> backup/{}",
                            target_dir.display(),
                            backup_name
                        );
                        let _dst = move_dir_to_backup(&target_dir, &backups_dir, &backup_name)?;
                    }
                }
            }
            "processmaster" => {
                let pm = config.processmaster.as_ref().ok_or_else(|| anyhow::anyhow!("processmaster config missing"))?;
                let app_name = format!("fleetman-{}", &c.cell_id);
                let service_dir = PathBuf::from(&pm.auto_services_root).join(&app_name);
                let client = processmaster_client::ProcessMasterClient::new(
                    PathBuf::from(&pm.pmctl_path),
                    pm.sock.clone(),
                );
                info!("üßπ Cell removed: pmctl stop {} (best-effort)", app_name);
                let _ = client.stop(&app_name);

                if service_dir.exists() {
                    let ts = now_ts_for_backup();
                    let backup_name = format!("fleetman-{}-{}", c.cell_id, ts);
                    info!(
                        "üßπ Cell removed: moving {} -> backup/{}",
                        service_dir.display(),
                        backup_name
                    );
                    let _dst = move_dir_to_backup(&service_dir, &backups_dir, &backup_name)?;
                }

                info!("üßπ Cell removed: pmctl update");
                client.update().ok();
            }
            "systemd" => {
                let systemd_cfg = config.systemd.as_ref().ok_or_else(|| anyhow::anyhow!("systemd config missing"))?;
                let service_name = format!("fleetman-{}", &c.cell_id);
                let service_dir = PathBuf::from(&systemd_cfg.working_directory).join(&service_name);

                info!("üßπ Cell removed: systemctl stop {} (best-effort)", service_name);
                let _ = stop_systemd_service(systemd_cfg, &service_name);

                if service_dir.exists() {
                    let ts = now_ts_for_backup();
                    let backup_name = format!("fleetman-{}-{}", c.cell_id, ts);
                    info!(
                        "üßπ Cell removed: moving {} -> backup/{}",
                        service_dir.display(),
                        backup_name
                    );
                    let _dst = move_dir_to_backup(&service_dir, &backups_dir, &backup_name)?;
                }

                // Disable and remove unit file, then daemon-reload.
                let _ = Command::new(&systemd_cfg.systemctl_path())
                    .args(&["disable", &service_name])
                    .output();

                let unit_path = PathBuf::from(&systemd_cfg.service_folder).join(format!("{}.service", service_name));
                if unit_path.exists() {
                    let _ = fs::remove_file(&unit_path);
                }
                let _ = systemd_daemon_reload(systemd_cfg);
            }
            _ => {}
        }

        // Remove from agent state so UI doesn't show stale entries.
        {
            let mut st = shared_state.lock().unwrap();
            st.remove_cell(&c.cell_id);
        }
    }

    Ok(())
}

#[derive(Debug, Clone)]
struct CellInfo {
    cell_id: String,
    manifest_name: String,
}

async fn discover_cells(config: &AgentConfig, s3_client: &S3Client) -> Result<Vec<CellInfo>> {
    let prefix = config
        .object_storage
        .full_key(&format!("cells/{}/", config.global.node_id));

    info!("üìÇ Listing S3: bucket={}, prefix={}", config.object_storage.bucket, prefix);

    let mut cells = Vec::new();

    let mut continuation_token: Option<String> = None;

    loop {
        let mut request = s3_client
            .list_objects_v2()
            .bucket(&config.object_storage.bucket)
            .prefix(&prefix)
            .delimiter("/");

        if let Some(token) = continuation_token {
            request = request.continuation_token(token);
        }

        debug!("Sending S3 ListObjectsV2 request...");
        let response = retry_s3_operation(
            "list_cells",
            config.global.s3_retry_count,
            config.global.s3_retry_delay_ms,
            || async { request.clone().send().await },
        ).await.map_err(|e| {
            error!("‚ùå S3 ListObjectsV2 failed: {}", e);
            anyhow::anyhow!("Failed to list cells: {:?}", e)
        })?;

        // Process common prefixes (cell directories)
        let common_prefixes_count = response.common_prefixes().len();
        debug!("Found {} common prefixes in this batch", common_prefixes_count);
        
        for cp in response.common_prefixes() {
                if let Some(prefix_str) = cp.prefix() {
                    debug!("Processing prefix: {}", prefix_str);
                    
                    // Strip bucket prefix
                    let prefix_with_slash = format!("{}/", config.object_storage.prefix);
                    let stripped = prefix_str.strip_prefix(&prefix_with_slash).unwrap_or(prefix_str);
                    
                    debug!("Stripped prefix: {}", stripped);
                    
                    // Extract cell_id from prefix: cells/<node_id>/<cell_id>/
                    let parts: Vec<&str> = stripped.trim_end_matches('/').split('/').collect();
                    debug!("Path parts: {:?}", parts);
                    if parts.len() >= 3 {
                        let cell_id = parts[2].to_string();
                        info!("üì¶ Found cell directory: {}", cell_id);

                        // Read cell metadata to get manifest_name
                        match get_cell_manifest_name(config, s3_client, &cell_id).await {
                            Ok(manifest_name) => {
                                info!("   ‚îî‚îÄ manifest: {}", manifest_name);
                                cells.push(CellInfo {
                                    cell_id,
                                    manifest_name,
                                });
                            }
                            Err(e) => {
                                warn!("   ‚îî‚îÄ Failed to get metadata for cell {}: {}", cell_id, e);
                            }
                        }
                    }
                }
            }

        if response.is_truncated().unwrap_or(false) {
            continuation_token = response.next_continuation_token().map(|s| s.to_string());
        } else {
            break;
        }
    }

    Ok(cells)
}

#[derive(Debug, Deserialize)]
struct TriggerFile {
    version: String,
}

#[derive(Debug, Deserialize)]
struct ManifestSnapshot {
    profile: String,
    files: Vec<ManifestFile>,
    #[serde(default = "default_service_owner")]
    service_owner: String,
    #[serde(default = "default_service_group")]
    service_group: String,
    #[serde(default)]
    service_folder_owner: Option<String>,
    #[serde(default)]
    service_folder_group: Option<String>,
    #[serde(default = "default_service_folder_mode")]
    service_folder_mode: String,
    #[serde(default = "default_collect_logs")]
    collect_logs: bool,
    #[serde(default = "default_log_files")]
    log_files: Vec<String>,
    #[serde(default = "default_max_log_lines")]
    max_log_lines: usize,
}

fn default_service_owner() -> String {
    "root".to_string()
}

fn default_service_group() -> String {
    "root".to_string()
}

fn default_service_folder_mode() -> String {
    "0700".to_string()
}

fn default_collect_logs() -> bool {
    true
}

fn default_log_files() -> Vec<String> {
    vec![
        "logs/stdout.log".to_string(),
        "logs/stderr.log".to_string(),
    ]
}

fn default_max_log_lines() -> usize {
    30
}

#[derive(Debug, Deserialize, Clone)]
struct ManifestFile {
    path: String,
    file_type: String,  // "binary", "text", "template", or "folder"
    owner: Option<String>,
    group: Option<String>,
    mode: Option<String>,
}

fn resolve_service_folder_owner_group(m: &ManifestSnapshot) -> (String, String) {
    let owner = m
        .service_folder_owner
        .as_deref()
        .map(|s| s.trim())
        .filter(|s| !s.is_empty())
        .unwrap_or(m.service_owner.trim());
    let group = m
        .service_folder_group
        .as_deref()
        .map(|s| s.trim())
        .filter(|s| !s.is_empty())
        .unwrap_or(m.service_group.trim());
    (owner.to_string(), group.to_string())
}

fn chown_recursive(path: &Path, owner: &str, group: &str) -> Result<()> {
    let output = Command::new("chown")
        .arg("-R")
        .arg(format!("{}:{}", owner, group))
        .arg(path)
        .output()
        .with_context(|| format!("Failed to execute chown -R for {:?}", path))?;
    if !output.status.success() {
        return Err(anyhow::anyhow!(
            "chown -R failed for {:?}: {}",
            path,
            String::from_utf8_lossy(&output.stderr)
        ));
    }
    Ok(())
}

fn reapply_manifest_permissions(target_dir: &Path, manifest: &ManifestSnapshot) -> Result<()> {
    let (svc_folder_owner, svc_folder_group) = resolve_service_folder_owner_group(manifest);
    for f in &manifest.files {
        let p = target_dir.join(&f.path);
        if !p.exists() {
            continue;
        }
        apply_permissions(&p, f, &svc_folder_owner, &svc_folder_group)?;
    }
    Ok(())
}

async fn get_cell_manifest_name(
    config: &AgentConfig,
    s3_client: &S3Client,
    cell_id: &str,
) -> Result<String> {
    let trigger_key = config
        .object_storage
        .full_key(&format!("cells/{}/{}/trigger.json", config.global.node_id, cell_id));

    let obj = retry_s3_operation(
        &format!("get_trigger_{}", cell_id),
        config.global.s3_retry_count,
        config.global.s3_retry_delay_ms,
        || async {
            s3_client
                .get_object()
                .bucket(&config.object_storage.bucket)
                .key(&trigger_key)
                .send()
                .await
        },
    ).await
    .with_context(|| format!("Failed to get trigger file for cell {}", cell_id))?;

    let body = obj.body.collect().await?.into_bytes();
    let _trigger: TriggerFile = serde_json::from_slice(&body)?;

    // Extract manifest_name from the path structure
    // Trigger points to a version, we need to find which manifest it belongs to
    // We'll need to list and find the manifest for this cell
    // For now, let's derive it from the cell's path structure

    // Actually, we should store manifest_name in the cell's metadata
    // Let's check for a cell_metadata.json file
    let metadata_key = config.object_storage.full_key(&format!(
        "cells/{}/{}/metadata.json",
        config.global.node_id, cell_id
    ));

    match s3_client
        .get_object()
        .bucket(&config.object_storage.bucket)
        .key(&metadata_key)
        .send()
        .await
    {
        Ok(obj) => {
            let body = obj.body.collect().await?.into_bytes();
            #[derive(Deserialize)]
            struct CellMetadata {
                manifest_name: String,
            }
            let metadata: CellMetadata = serde_json::from_slice(&body)?;
            Ok(metadata.manifest_name)
        }
        Err(_) => {
            // Fallback: try to infer from directory listing
            // This is a workaround if metadata doesn't exist
            Ok(cell_id.to_string())
        }
    }
}

async fn process_cell(
    config: &AgentConfig,
    s3_client: &S3Client,
    shared_state: &SharedState,
    cell_info: &CellInfo,
) -> Result<()> {
    info!("üìã Processing cell: {}", cell_info.cell_id);
    
    // Get trigger file
    let trigger_key = config.object_storage.full_key(&format!(
        "cells/{}/{}/trigger.json",
        config.global.node_id, cell_info.cell_id
    ));

    debug!("Reading trigger file: {}", trigger_key);
    let obj = retry_s3_operation(
        &format!("get_trigger_{}", cell_info.cell_id),
        config.global.s3_retry_count,
        config.global.s3_retry_delay_ms,
        || async {
            s3_client
                .get_object()
                .bucket(&config.object_storage.bucket)
                .key(&trigger_key)
                .send()
                .await
        },
    ).await
    .with_context(|| format!("Failed to get trigger.json for cell {} from S3", cell_info.cell_id))?;

    let last_modified = obj
        .last_modified()
        .map(|dt| dt.secs() as u64)
        .unwrap_or(0);

    let body = obj.body.collect().await
        .with_context(|| format!("Failed to read trigger.json body for cell {}", cell_info.cell_id))?
        .into_bytes();
    
    let trigger: TriggerFile = serde_json::from_slice(&body)
        .with_context(|| format!("Failed to parse trigger.json for cell {}: invalid JSON", cell_info.cell_id))?;
    
    info!("   Trigger version: {}, timestamp: {}", trigger.version, last_modified);

    // Check if this is a new deployment
    let should_deploy = {
        let state = shared_state.lock().unwrap();
        match state.get_cell(&cell_info.cell_id) {
            Some(cell_state) => {
                // Only check if trigger timestamp changed
                // Don't re-process same trigger even if current version differs (could be due to rollback)
                cell_state.trigger_timestamp != Some(last_modified)
            }
            None => true, // New cell, deploy
        }
    };

    if !should_deploy {
        info!(
            "‚úì Cell {} already processed trigger (timestamp: {}), skipping",
            cell_info.cell_id, last_modified
        );
        return Ok(());
    }

    info!(
        "üîÑ Deploying cell {} to version {}",
        cell_info.cell_id, trigger.version
    );

    // Read manifest.json *from the cell version folder* so deploy/rollback is fully determined
    // by the per-cell snapshot (base deployment + merged overrides).
    debug!("Reading manifest.json from cell version folder for version {}", trigger.version);
    let cell_manifest_key_base = format!(
        "cells/{}/{}/versions/{}/manifest.json",
        config.global.node_id,
        cell_info.cell_id,
        trigger.version
    );
    let cell_manifest_key = config.object_storage.full_key(&cell_manifest_key_base);

    let manifest_obj = retry_s3_operation(
        &format!("get_cell_manifest_{}_{}", cell_info.cell_id, trigger.version),
        config.global.s3_retry_count,
        config.global.s3_retry_delay_ms,
        || async {
            s3_client
                .get_object()
                .bucket(&config.object_storage.bucket)
                .key(&cell_manifest_key)
                .send()
                .await
        },
    )
    .await
    .with_context(|| {
        format!(
            "Failed to download cell manifest.json for cell {} version {}",
            cell_info.cell_id, trigger.version
        )
    })?;

    let manifest_body = manifest_obj.body.collect().await?.into_bytes();
    let manifest: ManifestSnapshot = serde_json::from_slice(&manifest_body)
        .with_context(|| format!("Failed to parse cell manifest.json for version {}", trigger.version))?;
    
    info!("   Deployment profile: {}", manifest.profile);
    info!("   Files in manifest: {}", manifest.files.len());

    // Update state to InProgress
    {
        let mut state = shared_state.lock().unwrap();
        let current_state = state.get_cell(&cell_info.cell_id).cloned();
        let new_state = CellState {
            cell_id: cell_info.cell_id.clone(),
            manifest_name: cell_info.manifest_name.clone(),
            profile: manifest.profile.clone(),
            current_version: current_state.as_ref().and_then(|s| s.current_version.clone()),
            previous_version: current_state.as_ref().and_then(|s| s.current_version.clone()),
            trigger_timestamp: Some(last_modified),
            last_deployment_status: DeploymentStatus::InProgress,
            last_deployment_time: Some(current_timestamp()),
            running: None,
        };
        state.update_cell(cell_info.cell_id.clone(), new_state);
    }

    // Execute deployment
    let deployment_result = match manifest.profile.as_str() {
        "systemd" | "Systemd" => {
            deploy_with_systemd(config, s3_client, cell_info, &trigger.version, &manifest).await
                .with_context(|| format!("Systemd deployment failed for cell {} version {}", 
                                        cell_info.cell_id, trigger.version))
        }
        "processmaster" | "ProcessMaster" | "process_master" | "process-master" => {
            deploy_with_processmaster(config, s3_client, cell_info, &trigger.version, &manifest).await
                .with_context(|| format!("ProcessMaster deployment failed for cell {} version {}",
                                        cell_info.cell_id, trigger.version))
        }
        "deploy" | "Deploy" => {
            deploy_with_simple(config, s3_client, cell_info, &trigger.version, &manifest).await
                .with_context(|| format!("Deploy deployment failed for cell {} version {}", 
                                        cell_info.cell_id, trigger.version))
        }
        _ => Err(anyhow::anyhow!("Unknown profile '{}' for cell {}", manifest.profile, cell_info.cell_id)),
    };

    // Update state based on result and handle rollback if needed
    match deployment_result {
        Ok(_) => {
            let timestamp = current_timestamp();
            let mut state = shared_state.lock().unwrap();
            if let Some(mut cell_state) = state.get_cell(&cell_info.cell_id).cloned() {
                info!("‚úÖ Successfully deployed {} to {}", cell_info.cell_id, trigger.version);
                cell_state.current_version = Some(trigger.version.clone());
                cell_state.last_deployment_status = DeploymentStatus::Success;
                cell_state.last_deployment_time = Some(timestamp);
                // Running status: known for service profiles, N/A for deploy.
                cell_state.running = match manifest.profile.to_lowercase().as_str() {
                    "deploy" => None,
                    _ => Some(true),
                };
                state.update_cell(cell_info.cell_id.clone(), cell_state);
            }
            drop(state); // Release lock before async operation
            
            // Write status.json to S3 (no logs on success)
            write_status_json(config, s3_client, cell_info, &trigger.version, timestamp, None, None).await
                .unwrap_or_else(|e| error!("Failed to write status.json for {}: {}", cell_info.cell_id, e));
            
            Ok(())
        }
        Err(e) => {
            let error_message = format!("{:#}", e);
            error!("‚ùå Failed to deploy {} to {}: {}", cell_info.cell_id, trigger.version, error_message);
            
            // Check if we should attempt rollback
            let previous_version = {
                let state = shared_state.lock().unwrap();
                state.get_cell(&cell_info.cell_id)
                    .and_then(|s| s.previous_version.clone())
            };
            
            let (final_version, final_error) = if let Some(prev_version) = previous_version {
                if !config.global.rollback_on_failure {
                    warn!(
                        "üß∑ Rollback disabled (global.rollback_on_failure=false). Leaving failed deployment in place for investigation."
                    );
                    // Update state to failed (keep failed version in place)
                    let timestamp = current_timestamp();
                    let mut state = shared_state.lock().unwrap();
                    if let Some(mut cell_state) = state.get_cell(&cell_info.cell_id).cloned() {
                        cell_state.current_version = Some(trigger.version.clone());
                        cell_state.last_deployment_status = DeploymentStatus::Failed;
                        cell_state.last_deployment_time = Some(timestamp);
                        // Running status: known for service profiles, N/A for deploy.
                        cell_state.running = match manifest.profile.to_lowercase().as_str() {
                            "deploy" => None,
                            _ => Some(false),
                        };
                        state.update_cell(cell_info.cell_id.clone(), cell_state);
                    }
                    (trigger.version.clone(), format!("Failed to deploy {}: {}. Rollback disabled; leaving failed version in place.", trigger.version, error_message))
                } else {
                    warn!("üîÑ Attempting rollback to previous version: {}", prev_version);
                
                    // Try to rollback to previous version
                    let rollback_result = match manifest.profile.as_str() {
                        "systemd" | "Systemd" => {
                            rollback_systemd(config, s3_client, cell_info, &prev_version).await
                        }
                        "processmaster" | "ProcessMaster" | "process_master" | "process-master" => {
                            rollback_processmaster(config, s3_client, cell_info, &prev_version).await
                        }
                        _ => Err(anyhow::anyhow!("Unknown profile for rollback")),
                    };
                
                    // Update state based on rollback result
                    let timestamp = current_timestamp();
                    let mut state = shared_state.lock().unwrap();
                    if let Some(mut cell_state) = state.get_cell(&cell_info.cell_id).cloned() {
                        match rollback_result {
                            Ok(_) => {
                                warn!("‚úÖ Successfully rolled back {} to {}", cell_info.cell_id, prev_version);
                                cell_state.current_version = Some(prev_version.clone());
                                cell_state.last_deployment_status = DeploymentStatus::Failed; // Keep failed status for new version
                                cell_state.last_deployment_time = Some(timestamp);
                                state.update_cell(cell_info.cell_id.clone(), cell_state);
                                (prev_version, format!("Failed to deploy {}: {}. Rolled back successfully.", trigger.version, error_message))
                            }
                            Err(rollback_err) => {
                                let rollback_error = format!("{:#}", rollback_err);
                                error!("‚ùå Rollback also failed for {}: {}", cell_info.cell_id, rollback_error);
                                cell_state.last_deployment_status = DeploymentStatus::Failed;
                                cell_state.last_deployment_time = Some(timestamp);
                                state.update_cell(cell_info.cell_id.clone(), cell_state);
                                (prev_version.clone(), format!("Failed to deploy {}: {}. Rollback to {} also failed: {}", trigger.version, error_message, prev_version, rollback_error))
                            }
                        }
                    } else {
                        (prev_version, error_message)
                    }
                }
            } else {
                warn!("‚ö†Ô∏è  No previous version to rollback to for {}", cell_info.cell_id);
                
                // Update state to failed
                let timestamp = current_timestamp();
                let mut state = shared_state.lock().unwrap();
                if let Some(mut cell_state) = state.get_cell(&cell_info.cell_id).cloned() {
                    cell_state.last_deployment_status = DeploymentStatus::Failed;
                    cell_state.last_deployment_time = Some(timestamp);
                    state.update_cell(cell_info.cell_id.clone(), cell_state);
                }
                
                ("unknown".to_string(), format!("Failed to deploy {}: {}. No previous version to rollback to.", trigger.version, error_message))
            };
            
            // Collect logs if enabled in manifest
            let logs = if let Some(service_dir) = get_service_directory(config, &cell_info.cell_id, &manifest.profile) {
                collect_logs(&service_dir, &manifest)
            } else {
                None
            };
            
            // Write status.json with failure details and logs
            let timestamp = current_timestamp();
            write_status_json(config, s3_client, cell_info, &final_version, timestamp, Some(&final_error), logs).await
                .unwrap_or_else(|err| error!("Failed to write status.json for {}: {}", cell_info.cell_id, err));
            
            Err(e)
        }
    }
}

async fn deploy_with_systemd(
    config: &AgentConfig,
    s3_client: &S3Client,
    cell_info: &CellInfo,
    version: &str,
    manifest: &ManifestSnapshot,
) -> Result<()> {
    let systemd_config = config
        .systemd
        .as_ref()
        .ok_or_else(|| anyhow::anyhow!("Systemd config not found"))?;

    // Folder name must match service name (for consistency with ProcessMaster auto-services).
    let service_name = format!("fleetman-{}", &cell_info.cell_id);
    let service_dir = format!("{}/{}", systemd_config.working_directory, &service_name);

    info!("1Ô∏è‚É£ Stopping service: {}", service_name);
    stop_systemd_service(systemd_config, &service_name)
        .with_context(|| format!("Failed to stop systemd service {}", service_name))?;

    info!("2Ô∏è‚É£ Verifying service stopped");
    verify_systemd_stopped(systemd_config, &service_name)
        .with_context(|| format!("Service {} failed to stop", service_name))?;

    info!("3Ô∏è‚É£ Downloading artifacts to {}", service_dir);
    download_version_files(config, s3_client, cell_info, version, &service_dir, manifest).await
        .with_context(|| format!("Failed to download version {} files to {}", version, service_dir))?;

    // Apply fleetagent_* template tokens before ownership/permissions.
    let working_dir_abs = match fs::canonicalize(&service_dir) {
        Ok(p) => p.to_string_lossy().to_string(),
        Err(_) => service_dir.clone(),
    };
    let service_dir_path = PathBuf::from(&service_dir);
    apply_templates_from_manifest(&service_dir_path, &cell_info.cell_id, &working_dir_abs, manifest)?;
    apply_templates_from_overrides_manifest(&service_dir_path, &cell_info.cell_id, &working_dir_abs)?;
    // These are commonly needed even when run.sh/app.service are plain text (not type=template).
    apply_fleetagent_tokens_in_file(&PathBuf::from(&service_dir).join("run.sh"), &cell_info.cell_id, &working_dir_abs)?;
    apply_fleetagent_tokens_in_file(&PathBuf::from(&service_dir).join("app.service"), &cell_info.cell_id, &working_dir_abs)?;
    // Ensure run-as identity converges to manifest on every deploy (even if app.service was edited).
    apply_service_identity_in_file(
        &PathBuf::from(&service_dir).join("app.service"),
        &manifest.service_owner,
        &manifest.service_group,
    )?;

    // Re-apply ownership/permissions on every redeploy.
    let service_dir_path = PathBuf::from(&service_dir);
    let (svc_folder_owner, svc_folder_group) = resolve_service_folder_owner_group(manifest);
    info!(
        "   Reapplying service folder ownership recursively: {}:{}: {}",
        svc_folder_owner,
        svc_folder_group,
        service_dir
    );
    chown_recursive(&service_dir_path, &svc_folder_owner, &svc_folder_group)?;
    reapply_manifest_permissions(&service_dir_path, manifest)?;

    // Overrides are snapshotted & merged into versions/<ver>/ at publish time.
    // Apply override permissions from overrides_manifest.json if present in the deployed folder.
    apply_overrides_manifest_in_place(&service_dir, &manifest.service_owner, &manifest.service_group)?;

    // Ensure run.sh is executable if present.
    let run_sh = PathBuf::from(&service_dir).join("run.sh");
    if run_sh.exists() {
        let mut perms = fs::metadata(&run_sh)?.permissions();
        perms.set_mode(0o755);
        fs::set_permissions(&run_sh, perms)?;
    }

    info!("5Ô∏è‚É£ Installing systemd service from app.service");
    let unit_src = PathBuf::from(&service_dir).join("app.service");
    if !unit_src.exists() {
        anyhow::bail!("Required file not found: {}", unit_src.display());
    }
    let unit_dst = PathBuf::from(&systemd_config.service_folder).join(format!("{}.service", service_name));
    let unit_content = fs::read_to_string(&unit_src)
        .with_context(|| format!("Failed to read {:?}", unit_src))?;
    fs::write(&unit_dst, unit_content.as_bytes())
        .with_context(|| format!("Failed to write systemd service {:?}", unit_dst))?;

    info!("6Ô∏è‚É£ Running systemctl daemon-reload");
    systemd_daemon_reload(systemd_config)
        .with_context(|| "Failed to reload systemd daemon")?;

    info!("7Ô∏è‚É£ Enabling service: {}", service_name);
    let output = Command::new(&systemd_config.systemctl_path())
        .args(&["enable", &service_name])
        .output()?;
    if !output.status.success() {
        warn!(
            "systemctl enable warning: {}",
            String::from_utf8_lossy(&output.stderr)
        );
    }

    info!("8Ô∏è‚É£ Starting service: {}", service_name);
    start_systemd_service(systemd_config, &service_name)
        .with_context(|| format!("Failed to start systemd service {}", service_name))?;

    info!("9Ô∏è‚É£ Verifying service running (waiting 5s)");
    tokio::time::sleep(Duration::from_secs(5)).await;
    verify_systemd_running(systemd_config, &service_name)
        .with_context(|| format!("Service {} is not running after 5s", service_name))?;

    // Write deployed-folder manifest for heartbeat + restart reconciliation.
    write_deployed_manifest(&service_dir, &cell_info.manifest_name, version, &manifest.profile)?;

    Ok(())
}

async fn download_version_files(
    config: &AgentConfig,
    s3_client: &S3Client,
    cell_info: &CellInfo,
    version: &str,
    target_dir: &str,
    manifest: &ManifestSnapshot,
) -> Result<()> {
    // Create target directory
    fs::create_dir_all(target_dir)?;

    let (svc_folder_owner, svc_folder_group) = resolve_service_folder_owner_group(manifest);
    
    // Apply service folder ownership/mode (systemd/processmaster only).
    let profile_lc = manifest.profile.to_ascii_lowercase();
    let is_svc_profile = profile_lc == "systemd" || profile_lc == "processmaster";
    if is_svc_profile {
        let target_path = Path::new(target_dir);
        let mode_val = u32::from_str_radix(manifest.service_folder_mode.trim(), 8)
            .with_context(|| format!("Invalid service_folder_mode '{}'", manifest.service_folder_mode))?;
        let mut perms = fs::metadata(target_path)?.permissions();
        perms.set_mode(mode_val);
        fs::set_permissions(target_path, perms)?;
        info!(
            "   Set service folder permissions to {}: {}",
            manifest.service_folder_mode, target_dir
        );

        // Owner/group (best-effort).
        info!("   Set service folder ownership to {}:{}: {}", svc_folder_owner, svc_folder_group, target_dir);
        let output = Command::new("chown")
            .arg(format!("{}:{}", svc_folder_owner, svc_folder_group))
            .arg(target_path)
            .output()
            .with_context(|| format!("Failed to execute chown for {:?}", target_path))?;
        if !output.status.success() {
            return Err(anyhow::anyhow!(
                "chown failed for {:?}: {}",
                target_path,
                String::from_utf8_lossy(&output.stderr)
            ));
        }
    } else {
        // Keep previous behavior for deploy/other profiles.
        let target_path = Path::new(target_dir);
        let mut perms = fs::metadata(target_path)?.permissions();
        perms.set_mode(0o755);
        fs::set_permissions(target_path, perms)?;
        info!("   Set service folder permissions to 0755: {}", target_dir);
    }

    let version_prefix = config.object_storage.full_key(&format!(
        "cells/{}/{}/versions/{}/",
        config.global.node_id, cell_info.cell_id, version
    ));

    info!("   Processing {} items from manifest", manifest.files.len());

    // Create folders first
    for file in &manifest.files {
        if file.file_type == "folder" {
            let folder_path = PathBuf::from(target_dir).join(&file.path);
            info!("   Creating folder: {}", file.path);
            fs::create_dir_all(&folder_path)
                .with_context(|| format!("Failed to create folder {}", file.path))?;
            
            // Apply folder permissions
            apply_permissions(&folder_path, &file, &svc_folder_owner, &svc_folder_group)?;
        }
    }

    // Download all files
    download_s3_directory(
        config,
        s3_client,
        &config.object_storage.bucket,
        &version_prefix,
        target_dir,
    )
    .await?;

    // Apply permissions to all files based on manifest
    for file in &manifest.files {
        if file.file_type != "folder" {
            let file_path = PathBuf::from(target_dir).join(&file.path);
            if file_path.exists() {
                debug!("   Setting permissions for: {}", file.path);
                apply_permissions(&file_path, &file, &svc_folder_owner, &svc_folder_group)?;
            } else {
                warn!("   File not found, skipping permission setting: {}", file.path);
            }
        }
    }

    Ok(())
}

// NOTE: With override snapshot+merge at publish time, the agent no longer downloads live overrides.
// It only applies permissions from `overrides_manifest.json` if present in the version folder.
//
// The below defaults + legacy helpers are kept for backward compatibility and debugging, but are
// no longer used by the main deployment flow.
#[allow(dead_code)]
const DEFAULT_OVERRIDE_OWNER: &str = "root";
#[allow(dead_code)]
const DEFAULT_OVERRIDE_GROUP: &str = "root";
#[allow(dead_code)]
const DEFAULT_OVERRIDE_MODE_FILE: &str = "0644";
#[allow(dead_code)]
const DEFAULT_OVERRIDE_MODE_FOLDER: &str = "0755";

/// Apply override permissions in-place using a snapshotted `overrides_manifest.json`
/// that the controller copies into the deployed folder at publish time.
///
/// Expected location: <service_dir>/overrides_manifest.json
fn default_override_mode(path: &str, file_type: &str) -> &'static str {
    let is_folder = file_type == "folder" || path.ends_with('/');
    let is_run_sh = path == "run.sh" || path.ends_with("/run.sh");
    if is_folder || is_run_sh {
        "0700"
    } else {
        "0600"
    }
}

fn apply_overrides_manifest_in_place(service_dir: &str, default_owner: &str, default_group: &str) -> Result<()> {
    let manifest_path = PathBuf::from(service_dir).join("overrides_manifest.json");
    if !manifest_path.exists() {
        return Ok(());
    }

    let content = fs::read_to_string(&manifest_path)
        .with_context(|| format!("Failed to read overrides manifest at {:?}", manifest_path))?;
    let manifest: CellOverridesManifest = serde_json::from_str(&content)
        .with_context(|| format!("Failed to parse overrides manifest at {:?}", manifest_path))?;

    // Ensure folders from manifest exist first
    for f in &manifest.files {
        if f.file_type == "folder" || f.path.ends_with('/') {
            let folder_path = PathBuf::from(service_dir).join(&f.path);
            if !folder_path.exists() {
                fs::create_dir_all(&folder_path)
                    .with_context(|| format!("Failed to create override folder {:?}", folder_path))?;
            }
        }
    }

    // Apply metadata for all files listed in manifest (only if they exist)
    for f in &manifest.files {
        let p = PathBuf::from(service_dir).join(&f.path);
        if !p.exists() {
            continue;
        }

        let mode_str = f
            .mode
            .as_deref()
            .map(|s| s.trim())
            .filter(|s| !s.is_empty())
            .unwrap_or_else(|| default_override_mode(&f.path, &f.file_type));
        let mode_val = u32::from_str_radix(mode_str.trim_start_matches('0'), 8)
            .with_context(|| format!("Invalid mode '{}' for {:?}", mode_str, p))?;
        let perms = fs::Permissions::from_mode(mode_val);
        fs::set_permissions(&p, perms)
            .with_context(|| format!("Failed to set mode {} for {:?}", mode_str, p))?;

        let owner = f
            .owner
            .as_deref()
            .map(|s| s.trim())
            .filter(|s| !s.is_empty())
            .unwrap_or(default_owner);
        let group = f
            .group
            .as_deref()
            .map(|s| s.trim())
            .filter(|s| !s.is_empty())
            .unwrap_or(default_group);
        let output = Command::new("chown")
            .arg(format!("{}:{}", owner, group))
            .arg(&p)
            .output()
            .with_context(|| format!("Failed to execute chown for {:?}", p))?;
        if !output.status.success() {
            return Err(anyhow::anyhow!(
                "chown failed for {:?}: {}",
                p,
                String::from_utf8_lossy(&output.stderr)
            ));
        }
    }

    // Best-effort defaults for override files not in manifest:
    // we *intentionally* do not walk the entire tree (could touch non-override files).
    Ok(())
}

#[derive(Debug, serde::Deserialize)]
struct CellOverridesManifest {
    files: Vec<CellOverrideFile>,
}

#[derive(Debug, serde::Deserialize)]
struct CellOverrideFile {
    path: String,
    file_type: String,
    #[serde(default)]
    owner: Option<String>,
    #[serde(default)]
    group: Option<String>,
    #[serde(default)]
    mode: Option<String>,
}

#[allow(dead_code)]
async fn download_cell_overrides(
    config: &AgentConfig,
    s3_client: &S3Client,
    cell_info: &CellInfo,
    target_dir: &str,
) -> Result<()> {
    // First, try to load the overrides manifest
    let manifest_key = config.object_storage.full_key(&format!(
        "cells/{}/{}/overrides_manifest.json",
        config.global.node_id, cell_info.cell_id
    ));
    
    let manifest: Option<CellOverridesManifest> = match retry_s3_operation(
        &format!("get_overrides_manifest_{}", cell_info.cell_id),
        config.global.s3_retry_count,
        config.global.s3_retry_delay_ms,
        || async {
            s3_client
                .get_object()
                .bucket(&config.object_storage.bucket)
                .key(&manifest_key)
                .send()
                .await
        },
    ).await
    {
        Ok(obj) => {
            match obj.body.collect().await {
                Ok(data) => {
                    match serde_json::from_slice::<CellOverridesManifest>(&data.to_vec()) {
                        Ok(m) => {
                            debug!("Loaded overrides manifest for {}", cell_info.cell_id);
                            Some(m)
                        }
                        Err(e) => {
                            warn!("Failed to parse overrides manifest: {}", e);
                            None
                        }
                    }
                }
                Err(e) => {
                    warn!("Failed to read overrides manifest body: {}", e);
                    None
                }
            }
        }
        Err(_) => {
            debug!("No overrides manifest found for {}", cell_info.cell_id);
            None
        }
    };
    
    let overrides_prefix = config.object_storage.full_key(&format!(
        "cells/{}/{}/overrides/",
        config.global.node_id, cell_info.cell_id
    ));

    // Check if overrides exist
    match retry_s3_operation(
        &format!("check_overrides_{}", cell_info.cell_id),
        config.global.s3_retry_count,
        config.global.s3_retry_delay_ms,
        || async {
            s3_client
                .list_objects_v2()
                .bucket(&config.object_storage.bucket)
                .prefix(&overrides_prefix)
                .max_keys(1)
                .send()
                .await
        },
    ).await
    {
        Ok(response) => {
            if !response.contents().is_empty() {
                let downloaded_files = download_s3_directory(
                    config,
                    s3_client,
                    &config.object_storage.bucket,
                    &overrides_prefix,
                    target_dir,
                )
                .await?;
                
                // Apply permissions from manifest (or defaults if no manifest)
                // ONLY to files that were actually downloaded as overrides
                apply_overrides_permissions(target_dir, manifest.as_ref(), &downloaded_files)?;
            } else {
                debug!("No overrides found for cell {}", cell_info.cell_id);
            }
        }
        Err(e) => {
            debug!("Failed to check for overrides: {}", e);
        }
    }

    Ok(())
}

#[allow(dead_code)]
fn apply_overrides_permissions(
    target_dir: &str,
    manifest: Option<&CellOverridesManifest>,
    downloaded_files: &[String],
) -> Result<()> {
    use std::os::unix::fs::PermissionsExt;
    
    // Build a map of paths to metadata from manifest (if exists)
    let mut manifest_map = std::collections::HashMap::new();
    if let Some(m) = manifest {
        for file_info in &m.files {
            manifest_map.insert(file_info.path.clone(), file_info);
        }
        
        // First, create folders defined in manifest
        for file_info in &m.files {
            if file_info.file_type == "folder" {
                let folder_path = PathBuf::from(target_dir).join(&file_info.path);
                if !folder_path.exists() {
                    fs::create_dir_all(&folder_path)
                        .with_context(|| format!("Failed to create folder {:?}", folder_path))?;
                }
            }
        }
    }
    
    // Only apply permissions to files that were actually downloaded as overrides
    for relative_path in downloaded_files {
        let file_path = PathBuf::from(target_dir).join(relative_path);
        
        // Skip if file doesn't exist
        if !file_path.exists() {
            continue;
        }
        
        // Determine owner, group, mode
        let (owner, group, mode) = if let Some(file_info) = manifest_map.get(relative_path) {
            // Use manifest metadata (or defaults if optional fields missing)
            let owner = file_info
                .owner
                .as_deref()
                .map(|s| s.trim())
                .filter(|s| !s.is_empty())
                .unwrap_or(DEFAULT_OVERRIDE_OWNER);
            let group = file_info
                .group
                .as_deref()
                .map(|s| s.trim())
                .filter(|s| !s.is_empty())
                .unwrap_or(DEFAULT_OVERRIDE_GROUP);
            let mode = file_info
                .mode
                .as_deref()
                .map(|s| s.trim())
                .filter(|s| !s.is_empty())
                .unwrap_or(DEFAULT_OVERRIDE_MODE_FILE);
            (owner, group, mode)
        } else {
            // Use defaults for files not in manifest
            let is_dir = file_path.is_dir();
            let default_mode = if is_dir { DEFAULT_OVERRIDE_MODE_FOLDER } else { DEFAULT_OVERRIDE_MODE_FILE };
            debug!("Applying default permissions to {:?} (not in manifest)", file_path);
            (DEFAULT_OVERRIDE_OWNER, DEFAULT_OVERRIDE_GROUP, default_mode)
        };
        
        // Apply permissions
        let mode_val = u32::from_str_radix(mode.trim_start_matches('0'), 8)
            .with_context(|| format!("Invalid mode '{}' for {:?}", mode, file_path))?;
        let perms = fs::Permissions::from_mode(mode_val);
        fs::set_permissions(&file_path, perms)
            .with_context(|| format!("Failed to set mode {} for {:?}", mode, file_path))?;
        
        // Apply ownership (ERROR SENSITIVE - must succeed)
        let output = Command::new("chown")
            .arg(format!("{}:{}", owner, group))
            .arg(&file_path)
            .output()
            .with_context(|| format!("Failed to execute chown for {:?}", file_path))?;
        
        if !output.status.success() {
            let stderr = String::from_utf8_lossy(&output.stderr);
            return Err(anyhow::anyhow!(
                "chown failed for {:?}: {}",
                file_path,
                stderr
            ));
        }
        
        debug!(
            "Applied override permissions: {:?} -> {}:{} ({})",
            file_path, owner, group, mode
        );
    }
    
    Ok(())
}

async fn download_s3_directory(
    config: &AgentConfig,
    s3_client: &S3Client,
    bucket: &str,
    prefix: &str,
    target_dir: &str,
) -> Result<Vec<String>> {
    let mut continuation_token: Option<String> = None;
    let mut downloaded_files = Vec::new();

    loop {
        let mut request = s3_client
            .list_objects_v2()
            .bucket(bucket)
            .prefix(prefix);

        if let Some(token) = continuation_token.clone() {
            request = request.continuation_token(token);
        }

        let response = retry_s3_operation(
            &format!("list_objects_{}", prefix),
            config.global.s3_retry_count,
            config.global.s3_retry_delay_ms,
            || async { request.clone().send().await },
        ).await?;

        for object in response.contents() {
                if let Some(key) = object.key() {
                    // Skip directories
                    if key.ends_with('/') {
                        continue;
                    }

                    // Calculate relative path
                    let relative_path = key.strip_prefix(prefix).unwrap_or(key);
                    let target_path = PathBuf::from(target_dir).join(relative_path);

                    // Create parent directories
                    if let Some(parent) = target_path.parent() {
                        fs::create_dir_all(parent)?;
                    }

                    // Download file
                    let key_clone = key.to_string();
                    let obj = retry_s3_operation(
                        &format!("download_{}", key),
                        config.global.s3_retry_count,
                        config.global.s3_retry_delay_ms,
                        || async {
                            s3_client.get_object().bucket(bucket).key(&key_clone).send().await
                        },
                    ).await?;

                    let body = obj.body.collect().await?.into_bytes();
                    fs::write(&target_path, body)?;

                    debug!("Downloaded: {} -> {:?}", key, target_path);
                    downloaded_files.push(relative_path.to_string());
                }
            }

        if response.is_truncated().unwrap_or(false) {
            continuation_token = response.next_continuation_token().map(|s| s.to_string());
        } else {
            break;
        }
    }

    Ok(downloaded_files)
}

// Supervisor support removed (replaced by processmaster).

async fn deploy_with_processmaster(
    config: &AgentConfig,
    s3_client: &S3Client,
    cell_info: &CellInfo,
    version: &str,
    manifest: &ManifestSnapshot,
) -> Result<()> {
    let pm = config
        .processmaster
        .as_ref()
        .ok_or_else(|| anyhow::anyhow!("processmaster config not found in agent_config"))?;

    let auto_root = PathBuf::from(&pm.auto_services_root);
    let app_name = format!("fleetman-{}", &cell_info.cell_id);
    // ProcessMaster auto-services require folder name == application name.
    let service_dir = auto_root.join(&app_name);
    let pm_provisioned_marker = service_dir.join(".pm_provisioned");

    // Stop first to avoid "text file busy" when updating run.sh / binaries.
    // This must be best-effort for first deployments where the app may not exist yet.
    let client = processmaster_client::ProcessMasterClient::new(
        PathBuf::from(&pm.pmctl_path),
        pm.sock.clone(),
    );
    info!("0Ô∏è‚É£ pmctl stop {} (best-effort)", app_name);
    if let Err(e) = client.stop(&app_name) {
        warn!("pmctl stop {} failed (continuing): {}", app_name, e);
    }

    info!("1Ô∏è‚É£ Downloading artifacts to {}", service_dir.display());
    download_version_files(
        config,
        s3_client,
        cell_info,
        version,
        service_dir.to_string_lossy().as_ref(),
        manifest,
    )
    .await
    .with_context(|| {
        format!(
            "Failed to download version {} files to {}",
            version,
            service_dir.display()
        )
    })?;

    // Apply fleetagent_* tokens for processmaster profile.
    // - fleetagent_cell_id -> cell id
    // - fleetagent_service_working_dir -> absolute path of the service dir
    // - fleetagent_base_dir -> agent process current working directory (absolute)
    let working_dir_abs = match fs::canonicalize(&service_dir) {
        Ok(p) => p.to_string_lossy().to_string(),
        Err(_) => {
            if service_dir.is_absolute() {
                service_dir.to_string_lossy().to_string()
            } else {
                std::env::current_dir()
                    .unwrap_or_else(|_| PathBuf::from("/"))
                    .join(&service_dir)
                    .to_string_lossy()
                    .to_string()
            }
        }
    };
    apply_templates_from_manifest(&service_dir, &cell_info.cell_id, &working_dir_abs, manifest)?;
    apply_templates_from_overrides_manifest(&service_dir, &cell_info.cell_id, &working_dir_abs)?;
    apply_fleetagent_tokens_in_file(&service_dir.join("run.sh"), &cell_info.cell_id, &working_dir_abs)?;
    apply_fleetagent_tokens_in_file(&service_dir.join("service.yml"), &cell_info.cell_id, &working_dir_abs)?;
    // Ensure run-as identity converges to manifest on every deploy (even if service.yml was edited).
    apply_service_identity_in_file(
        &service_dir.join("service.yml"),
        &manifest.service_owner,
        &manifest.service_group,
    )?;

    // Re-apply ownership/permissions on every redeploy (after templating).
    let (svc_folder_owner, svc_folder_group) = resolve_service_folder_owner_group(manifest);
    info!(
        "   Reapplying service folder ownership recursively: {}:{}: {}",
        svc_folder_owner,
        svc_folder_group,
        service_dir.display()
    );
    chown_recursive(&service_dir, &svc_folder_owner, &svc_folder_group)?;
    reapply_manifest_permissions(&service_dir, manifest)?;

    // Overrides are snapshotted & merged into versions/<ver>/ at publish time.
    // Apply override permissions from overrides_manifest.json if present in the deployed folder.
    apply_overrides_manifest_in_place(
        service_dir.to_string_lossy().as_ref(),
        &manifest.service_owner,
        &manifest.service_group,
    )?;

    // Ensure run.sh is executable if present.
    let run_sh = service_dir.join("run.sh");
    if run_sh.exists() {
        let mut perms = fs::metadata(&run_sh)?.permissions();
        perms.set_mode(0o755);
        fs::set_permissions(&run_sh, perms)?;
    }

    // Force ProcessMaster to re-provision (e.g. to re-apply capabilities like net_bind_service)
    // by removing the marker that indicates the service folder has already been provisioned.
    if pm_provisioned_marker.exists() {
        info!(
            "   Removing ProcessMaster provision marker to force reprovision: {}",
            pm_provisioned_marker.display()
        );
        if let Err(e) = fs::remove_file(&pm_provisioned_marker) {
            warn!(
                "Failed to remove {:?} (continuing): {}",
                pm_provisioned_marker,
                e
            );
        }
    }

    // pmctl update + start
    info!("2Ô∏è‚É£ pmctl update");
    client.update()?;

    info!("3Ô∏è‚É£ pmctl start {}", app_name);
    client.start(&app_name)?;

    // Give ProcessMaster a short grace window to transition to running.
    tokio::time::sleep(Duration::from_secs(5)).await;
    info!("4Ô∏è‚É£ Verifying processmaster status (json)");
    let st = client.status_json(&app_name)?;
    if !st.ok {
        anyhow::bail!("pmctl status returned ok=false: {}", st.message);
    }
    let s0 = st
        .statuses
        .get(0)
        .ok_or_else(|| anyhow::anyhow!("pmctl status returned empty statuses list"))?;
    if !s0.running {
        anyhow::bail!(
            "ProcessMaster reports not running (actual={:?}, phase={:?}, last_exit_code={:?}, pids={:?})",
            s0.actual,
            s0.phase,
            s0.last_exit_code,
            s0.pids
        );
    }

    // Write deployed-folder manifest for heartbeat + restart reconciliation.
    write_deployed_manifest(
        service_dir.to_string_lossy().as_ref(),
        &cell_info.manifest_name,
        version,
        &manifest.profile,
    )?;

    Ok(())
}

async fn deploy_with_simple(
    config: &AgentConfig,
    s3_client: &S3Client,
    cell_info: &CellInfo,
    version: &str,
    manifest: &ManifestSnapshot,
) -> Result<()> {
    let deploy_config = config
        .deploy
        .as_ref()
        .ok_or_else(|| anyhow::anyhow!("Deploy config not found"))?;

    let target_dir = format!("{}/{}", deploy_config.target_folder, cell_info.cell_id);

    info!("1Ô∏è‚É£ Deploying files to {}", target_dir);
    download_version_files(config, s3_client, cell_info, version, &target_dir, manifest).await
        .with_context(|| format!("Failed to download version {} files to {}", version, target_dir))?;

    // Apply fleetagent_* tokens for any template files (and run.sh if present) before permissions.
    let working_dir_abs = match fs::canonicalize(&target_dir) {
        Ok(p) => p.to_string_lossy().to_string(),
        Err(_) => target_dir.clone(),
    };
    let target_dir_path = PathBuf::from(&target_dir);
    apply_templates_from_manifest(&target_dir_path, &cell_info.cell_id, &working_dir_abs, manifest)?;
    apply_templates_from_overrides_manifest(&target_dir_path, &cell_info.cell_id, &working_dir_abs)?;
    apply_fleetagent_tokens_in_file_optional(&PathBuf::from(&target_dir).join("run.sh"), &cell_info.cell_id, &working_dir_abs)?;

    // Overrides are snapshotted & merged into versions/<ver>/ at publish time.
    // Apply override permissions from overrides_manifest.json if present in the deployed folder.
    apply_overrides_manifest_in_place(&target_dir, &manifest.service_owner, &manifest.service_group)?;

    // Write deployed-folder manifest for heartbeat + restart reconciliation.
    write_deployed_manifest(&target_dir, &cell_info.manifest_name, version, &manifest.profile)?;

    info!("‚úÖ Deployment complete");
    Ok(())
}

// Supervisor support removed (replaced by processmaster).

// Systemd service management
fn systemd_daemon_reload(config: &SystemdConfig) -> Result<()> {
    info!("   Running systemctl daemon-reload");
    let output = Command::new(&config.systemctl_path())
        .args(&["daemon-reload"])
        .output()?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(anyhow::anyhow!(
            "systemctl daemon-reload failed: {}",
            stderr
        ));
    }

    Ok(())
}

fn stop_systemd_service(config: &SystemdConfig, service_name: &str) -> Result<()> {
    let output = Command::new(&config.systemctl_path())
        .args(&["stop", service_name])
        .output()?;

    // Don't fail if service is already stopped
    debug!(
        "Stop output: {}",
        String::from_utf8_lossy(&output.stdout)
    );

    Ok(())
}

fn verify_systemd_stopped(config: &SystemdConfig, service_name: &str) -> Result<()> {
    let output = Command::new(&config.systemctl_path())
        .args(&["is-active", service_name])
        .output()?;

    let status = String::from_utf8_lossy(&output.stdout);

    if status.contains("inactive") || status.contains("failed") || !output.status.success() {
        Ok(())
    } else {
        Err(anyhow::anyhow!(
            "Service {} is not stopped: {}",
            service_name,
            status
        ))
    }
}

fn start_systemd_service(config: &SystemdConfig, service_name: &str) -> Result<()> {
    let output = Command::new(&config.systemctl_path())
        .args(&["start", service_name])
        .output()?;

    if !output.status.success() {
        return Err(anyhow::anyhow!(
            "Failed to start service {}: {}",
            service_name,
            String::from_utf8_lossy(&output.stderr)
        ));
    }

    Ok(())
}

fn verify_systemd_running(config: &SystemdConfig, service_name: &str) -> Result<()> {
    let output = Command::new(&config.systemctl_path())
        .args(&["status", service_name])
        .output()?;

    let status = String::from_utf8_lossy(&output.stdout);

    if status.contains("Active:") && status.contains("running") {
        Ok(())
    } else {
        Err(anyhow::anyhow!(
            "Service {} is not running: {}",
            service_name,
            status
        ))
    }
}

fn init_tracing(global_config: &GlobalConfig) -> Option<tracing_appender::non_blocking::WorkerGuard> {
    use tracing_subscriber::layer::SubscriberExt;
    use tracing_subscriber::util::SubscriberInitExt;

    // Determine log level from RUST_LOG env var or default to "info"
    let log_level = std::env::var("RUST_LOG").unwrap_or_else(|_| "info".to_string());

    if let Some(ref log_dir) = global_config.log_directory {
        let file_prefix = global_config
            .log_file_prefix
            .as_deref()
            .unwrap_or("agent");

        let file_appender = match global_config.log_rotation.as_str() {
            "daily" => tracing_appender::rolling::daily(log_dir, file_prefix),
            "hourly" => tracing_appender::rolling::hourly(log_dir, file_prefix),
            _ => tracing_appender::rolling::never(log_dir, file_prefix),
        };

        let (non_blocking, guard) = tracing_appender::non_blocking(file_appender);

        tracing_subscriber::registry()
            .with(
                tracing_subscriber::fmt::layer()
                    .with_writer(non_blocking)
                    .with_ansi(false)
                    .with_target(false)
                    .with_thread_ids(false),
            )
            .with(tracing_subscriber::EnvFilter::new(&log_level))
            .init();
        
        eprintln!("‚úÖ Logging initialized to file: {}/{}", log_dir, file_prefix);
        
        Some(guard)
    } else {
        tracing_subscriber::fmt()
            .with_env_filter(&log_level)
            .with_target(false)
            .init();
        
        eprintln!("‚úÖ Logging initialized to stdout (level: {})", log_level);
        
        None
    }
}

fn apply_permissions(
    path: &Path,
    file: &ManifestFile,
    default_owner: &str,
    default_group: &str,
) -> Result<()> {
    // Default modes:
    // - run.sh and folders: 0700
    // - everything else: 0600
    let is_folder = file.file_type == "folder" || file.path.ends_with('/');
    let is_run_sh = file.path == "run.sh" || file.path.ends_with("/run.sh");
    let default_mode: u32 = if is_folder || is_run_sh { 0o700 } else { 0o600 };

    // Set file mode (permissions)
    let mode_val: u32 = match file.mode.as_deref().map(|s| s.trim()).filter(|s| !s.is_empty()) {
        Some(mode_str) => u32::from_str_radix(mode_str.trim_start_matches('0'), 8)
            .with_context(|| format!("Invalid mode '{}' for {}", mode_str, file.path))?,
        None => default_mode,
    };
    let permissions = fs::Permissions::from_mode(mode_val);
    fs::set_permissions(path, permissions)
        .with_context(|| format!("Failed to set mode {:o} for {}", mode_val, file.path))?;

    // Set owner and group using chown (defaulting to service folder identity).
    let owner = file
        .owner
        .as_deref()
        .map(|s| s.trim())
        .filter(|s| !s.is_empty())
        .unwrap_or(default_owner);
    let group = file
        .group
        .as_deref()
        .map(|s| s.trim())
        .filter(|s| !s.is_empty())
        .unwrap_or(default_group);
    let chown_arg = format!("{}:{}", owner, group);

    let output = Command::new("chown")
        .arg(&chown_arg)
        .arg(path)
        .output()
        .with_context(|| format!("Failed to execute chown for {}", file.path))?;
    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(anyhow::anyhow!("chown failed for {}: {}", file.path, stderr));
    }

    Ok(())
}

// Rollback functions
async fn rollback_processmaster(
    config: &AgentConfig,
    s3_client: &S3Client,
    cell_info: &CellInfo,
    rollback_version: &str,
) -> Result<()> {
    let pm = config
        .processmaster
        .as_ref()
        .ok_or_else(|| anyhow::anyhow!("processmaster config not found in agent_config"))?;

    let app_name = format!("fleetman-{}", &cell_info.cell_id);
    let service_dir = PathBuf::from(&pm.auto_services_root).join(&app_name);
    let pm_provisioned_marker = service_dir.join(".pm_provisioned");

    // Stop first to avoid "text file busy" when updating run.sh / binaries.
    // Best-effort for missing services.
    let client = processmaster_client::ProcessMasterClient::new(
        PathBuf::from(&pm.pmctl_path),
        pm.sock.clone(),
    );
    info!("üîô Rollback: pmctl stop {} (best-effort)", app_name);
    if let Err(e) = client.stop(&app_name) {
        warn!("pmctl stop {} failed (continuing): {}", app_name, e);
    }

    info!("üîô Rollback: Downloading version {}", rollback_version);

    // Load manifest from the cell version folder (snapshot source of truth)
    let manifest_key_base = format!(
        "cells/{}/{}/versions/{}/manifest.json",
        config.global.node_id,
        cell_info.cell_id,
        rollback_version
    );
    let full_manifest_key = config.object_storage.full_key(&manifest_key_base);

    let manifest_obj = s3_client
        .get_object()
        .bucket(&config.object_storage.bucket)
        .key(&full_manifest_key)
        .send()
        .await
        .with_context(|| {
            format!(
                "Failed to download cell manifest for rollback version {}",
                rollback_version
            )
        })?;

    let manifest_body = manifest_obj.body.collect().await?.into_bytes();
    let manifest: ManifestSnapshot =
        serde_json::from_slice(&manifest_body).with_context(|| "Failed to parse rollback manifest")?;

    download_version_files(
        config,
        s3_client,
        cell_info,
        rollback_version,
        service_dir.to_string_lossy().as_ref(),
        &manifest,
    )
    .await?;

    // Apply fleetagent_* tokens for processmaster profile.
    let working_dir_abs = match fs::canonicalize(&service_dir) {
        Ok(p) => p.to_string_lossy().to_string(),
        Err(_) => {
            if service_dir.is_absolute() {
                service_dir.to_string_lossy().to_string()
            } else {
                std::env::current_dir()
                    .unwrap_or_else(|_| PathBuf::from("/"))
                    .join(&service_dir)
                    .to_string_lossy()
                    .to_string()
            }
        }
    };
    apply_templates_from_manifest(&service_dir, &cell_info.cell_id, &working_dir_abs, &manifest)?;
    apply_templates_from_overrides_manifest(&service_dir, &cell_info.cell_id, &working_dir_abs)?;
    apply_fleetagent_tokens_in_file(&service_dir.join("run.sh"), &cell_info.cell_id, &working_dir_abs)?;
    apply_fleetagent_tokens_in_file(&service_dir.join("service.yml"), &cell_info.cell_id, &working_dir_abs)?;
    apply_service_identity_in_file(
        &service_dir.join("service.yml"),
        &manifest.service_owner,
        &manifest.service_group,
    )?;

    // Re-apply ownership/permissions on rollback as well (after templating).
    let (svc_folder_owner, svc_folder_group) = resolve_service_folder_owner_group(&manifest);
    info!(
        "üîô Rollback: reapplying service folder ownership recursively: {}:{}: {}",
        svc_folder_owner,
        svc_folder_group,
        service_dir.display()
    );
    chown_recursive(&service_dir, &svc_folder_owner, &svc_folder_group)?;
    reapply_manifest_permissions(&service_dir, &manifest)?;
    apply_overrides_manifest_in_place(
        service_dir.to_string_lossy().as_ref(),
        &manifest.service_owner,
        &manifest.service_group,
    )?;

    let run_sh = service_dir.join("run.sh");
    if run_sh.exists() {
        let mut perms = fs::metadata(&run_sh)?.permissions();
        perms.set_mode(0o755);
        fs::set_permissions(&run_sh, perms)?;
    }

    // Same as deploy: remove provision marker to force ProcessMaster to re-provision.
    if pm_provisioned_marker.exists() {
        info!(
            "üîô Rollback: removing ProcessMaster provision marker to force reprovision: {}",
            pm_provisioned_marker.display()
        );
        if let Err(e) = fs::remove_file(&pm_provisioned_marker) {
            warn!(
                "Failed to remove {:?} (continuing): {}",
                pm_provisioned_marker,
                e
            );
        }
    }

    info!("üîô Rollback: pmctl update");
    client.update()?;
    info!("üîô Rollback: pmctl start {}", app_name);
    client.start(&app_name)?;

    tokio::time::sleep(Duration::from_secs(5)).await;
    let st = client.status_json(&app_name)?;
    if !st.ok {
        anyhow::bail!("pmctl status returned ok=false: {}", st.message);
    }
    let s0 = st
        .statuses
        .get(0)
        .ok_or_else(|| anyhow::anyhow!("pmctl status returned empty statuses list"))?;
    if !s0.running {
        anyhow::bail!(
            "ProcessMaster reports not running after rollback (actual={:?}, phase={:?}, last_exit_code={:?}, pids={:?})",
            s0.actual,
            s0.phase,
            s0.last_exit_code,
            s0.pids
        );
    }

    // Write deployed-folder manifest for heartbeat + restart reconciliation.
    write_deployed_manifest(
        service_dir.to_string_lossy().as_ref(),
        &cell_info.manifest_name,
        rollback_version,
        &manifest.profile,
    )?;

    Ok(())
}

async fn rollback_systemd(
    config: &AgentConfig,
    s3_client: &S3Client,
    cell_info: &CellInfo,
    rollback_version: &str,
) -> Result<()> {
    let systemd_config = config
        .systemd
        .as_ref()
        .ok_or_else(|| anyhow::anyhow!("Systemd config not found"))?;

    let service_name = format!("fleetman-{}", &cell_info.cell_id);
    let service_dir = format!("{}/{}", systemd_config.working_directory, &service_name);

    info!("üîô Rollback: Stopping service");
    stop_systemd_service(systemd_config, &service_name)?;
    
    info!("üîô Rollback: Downloading version {}", rollback_version);
    
    // Load manifest from the cell version folder (snapshot source of truth)
    let manifest_key_base = format!(
        "cells/{}/{}/versions/{}/manifest.json",
        config.global.node_id,
        cell_info.cell_id,
        rollback_version
    );
    let full_manifest_key = config.object_storage.full_key(&manifest_key_base);

    let manifest_obj = s3_client
        .get_object()
        .bucket(&config.object_storage.bucket)
        .key(&full_manifest_key)
        .send()
        .await
        .with_context(|| format!("Failed to download cell manifest for rollback version {}", rollback_version))?;

    let manifest_body = manifest_obj.body.collect().await?.into_bytes();
    let manifest: ManifestSnapshot = serde_json::from_slice(&manifest_body)
        .with_context(|| "Failed to parse rollback manifest")?;
    
    // Download files
    download_version_files(config, s3_client, cell_info, rollback_version, &service_dir, &manifest).await?;
    
    // Apply fleetagent_* template tokens before ownership/permissions.
    let service_dir_path = PathBuf::from(&service_dir);
    let working_dir_abs = match fs::canonicalize(&service_dir) {
        Ok(p) => p.to_string_lossy().to_string(),
        Err(_) => service_dir.clone(),
    };
    apply_templates_from_manifest(&service_dir_path, &cell_info.cell_id, &working_dir_abs, &manifest)?;
    apply_templates_from_overrides_manifest(&service_dir_path, &cell_info.cell_id, &working_dir_abs)?;
    apply_fleetagent_tokens_in_file(&PathBuf::from(&service_dir).join("run.sh"), &cell_info.cell_id, &working_dir_abs)?;
    apply_fleetagent_tokens_in_file(&PathBuf::from(&service_dir).join("app.service"), &cell_info.cell_id, &working_dir_abs)?;
    apply_service_identity_in_file(
        &PathBuf::from(&service_dir).join("app.service"),
        &manifest.service_owner,
        &manifest.service_group,
    )?;

    // Re-apply ownership/permissions on rollback as well.
    let (svc_folder_owner, svc_folder_group) = resolve_service_folder_owner_group(&manifest);
    info!(
        "üîô Rollback: reapplying service folder ownership recursively: {}:{}: {}",
        svc_folder_owner,
        svc_folder_group,
        service_dir
    );
    chown_recursive(&service_dir_path, &svc_folder_owner, &svc_folder_group)?;
    reapply_manifest_permissions(&service_dir_path, &manifest)?;

    // Overrides are snapshotted & merged into versions/<ver>/ at publish time.
    apply_overrides_manifest_in_place(&service_dir, &manifest.service_owner, &manifest.service_group)?;

    // Ensure run.sh is executable if present.
    let run_sh = PathBuf::from(&service_dir).join("run.sh");
    if run_sh.exists() {
        let mut perms = fs::metadata(&run_sh)?.permissions();
        perms.set_mode(0o755);
        fs::set_permissions(&run_sh, perms)?;
    }

    // Copy unit file into /etc/systemd/system (or configured service folder).
    let unit_src = PathBuf::from(&service_dir).join("app.service");
    if !unit_src.exists() {
        anyhow::bail!("Required file not found: {}", unit_src.display());
    }
    let unit_dst = PathBuf::from(&systemd_config.service_folder).join(format!("{}.service", service_name));
    let unit_content = fs::read_to_string(&unit_src)
        .with_context(|| format!("Failed to read {:?}", unit_src))?;
    fs::write(&unit_dst, unit_content.as_bytes())
        .with_context(|| format!("Failed to write systemd service {:?}", unit_dst))?;
    
    info!("üîô Rollback: Running daemon-reload");
    systemd_daemon_reload(systemd_config)?;
    
    info!("üîô Rollback: Enabling service");
    let output = Command::new(&systemd_config.systemctl_path())
        .args(&["enable", &service_name])
        .output()?;
    if !output.status.success() {
        warn!(
            "systemctl enable warning: {}",
            String::from_utf8_lossy(&output.stderr)
        );
    }

    info!("üîô Rollback: Starting service");
    start_systemd_service(systemd_config, &service_name)?;
    
    info!("üîô Rollback: Verifying service (waiting 5s)");
    tokio::time::sleep(Duration::from_secs(5)).await;
    verify_systemd_running(systemd_config, &service_name)?;

    // Write deployed-folder manifest for heartbeat + restart reconciliation.
    write_deployed_manifest(&service_dir, &cell_info.manifest_name, rollback_version, &manifest.profile)?;

    Ok(())
}

/// Get the service directory based on the profile and config
fn get_service_directory(config: &AgentConfig, cell_id: &str, profile: &str) -> Option<String> {
    match profile.to_lowercase().as_str() {
        "systemd" => {
            // Folder name matches unit/service name: fleetman-<cell_id>
            config
                .systemd
                .as_ref()
                .map(|s| format!("{}/fleetman-{}", s.working_directory, cell_id))
        }
        "processmaster" | "process_master" | "process-master" => {
            config
                .processmaster
                .as_ref()
                .map(|p| format!("{}/fleetman-{}", p.auto_services_root, cell_id))
        }
        "deploy" => {
            config.deploy.as_ref().map(|d| format!("{}/{}", d.target_folder, cell_id))
        }
        _ => None,
    }
}

/// Collect logs from specified files in the service directory
/// Returns a JSON object with log file paths as keys and arrays of log lines as values
fn collect_logs(
    service_dir: &str,
    manifest: &ManifestSnapshot,
) -> Option<serde_json::Value> {
    if !manifest.collect_logs || manifest.log_files.is_empty() {
        return None;
    }
    
    let mut logs = serde_json::Map::new();
    
    for log_file in &manifest.log_files {
        // Validate log file path (no absolute paths, no escapes)
        if log_file.starts_with('/') || log_file.starts_with("../") || log_file.starts_with("./")
            || log_file.contains("/../") || log_file.contains("/./") {
            warn!("Skipping invalid log file path: {}", log_file);
            continue;
        }
        
        let log_path = PathBuf::from(service_dir).join(log_file);
        
        // Read the file and collect last N lines
        match std::fs::read_to_string(&log_path) {
            Ok(content) => {
                let lines: Vec<String> = content
                    .lines()
                    .rev()
                    .take(manifest.max_log_lines)
                    .map(|s| s.to_string())
                    .collect::<Vec<_>>()
                    .into_iter()
                    .rev()
                    .collect();
                
                logs.insert(log_file.clone(), serde_json::json!(lines));
                debug!("Collected {} lines from log file: {}", lines.len(), log_file);
            }
            Err(e) => {
                warn!("Failed to read log file {}: {}", log_file, e);
                logs.insert(log_file.clone(), serde_json::Value::Null);
            }
        }
    }
    
    if logs.is_empty() {
        None
    } else {
        Some(serde_json::Value::Object(logs))
    }
}

async fn write_status_json(
    config: &AgentConfig,
    s3_client: &S3Client,
    cell_info: &CellInfo,
    current_version: &str,
    timestamp: u64,
    error: Option<&str>,
    logs: Option<serde_json::Value>,
) -> Result<()> {
    let mut status = serde_json::json!({
        "running_version": current_version,
        "applied_at": timestamp,
        "status": if error.is_none() { "success" } else { "failed" },
        "error": error,
    });
    
    // Add logs if provided
    if let Some(logs_data) = logs {
        status.as_object_mut().unwrap().insert("logs".to_string(), logs_data);
    }
    
    let status_content = serde_json::to_string_pretty(&status)?;
    
    let status_key_base = format!("cells/{}/{}/status.json", config.global.node_id, cell_info.cell_id);
    let status_key = config.object_storage.full_key(&status_key_base);
    
    let status_bytes = status_content.into_bytes();
    retry_s3_operation(
        &format!("write_status_{}", cell_info.cell_id),
        config.global.s3_retry_count,
        config.global.s3_retry_delay_ms,
        || async {
            s3_client
                .put_object()
                .bucket(&config.object_storage.bucket)
                .key(&status_key)
                .body(ByteStream::from(status_bytes.clone()))
                .send()
                .await
        },
    ).await?;
    
    debug!("Wrote status.json for cell {}: version={}, status={}", 
           cell_info.cell_id, current_version, if error.is_none() { "success" } else { "failed" });
    
    Ok(())
}

fn current_timestamp() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .unwrap()
        .as_secs()
}

